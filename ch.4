선형 회귀

ⓐ.회귀 분석이란?
 : 결과변수와 원인변수들 사이의 함수 관계에 대한 통계적 분석
   (결과 변수) = f(원인 변수1, 원인 변수2, …) + 

ⓑ. 목적
 : 1.회귀분석을 통하여 회귀방정식 ( regression equation )을 명확하게 결정    하는 것
  2. 기별 설명 변수들의 중요성 평가
  3. 반응 변수의 값을 예측하기 위해

ⓒ. 이름의 유래
 : 키가 큰 아버지의 아들은 키가 크지만 아버지 만큼 크진 않고,
  키가 작은 아버지의 아들은 키가 작지만 아버지 만큼 작진 않다.
  -> 평균 선으로 회귀한다
  -> 회귀 분석


ⓔ 가정들

  1 .는 각각 독립이다.
  2. 
  3.  -> t,f분포로 추정하기 위해 필요한 가정

ⓗ 모델 및 변수의 유의성 확인
   > summary(lm2)
Call:
lm(formula = PM10_d ~ PM10_s + use + car + vel + l_temp + h_temp + 
    h_hum + h_wind + mise + chomise + weekend, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-19.6219  -2.9566  -0.2961   2.3524  23.7293 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) -4.551e+01  8.330e+00  -5.463 6.68e-08 ***
PM10_s       5.396e-01  1.487e-02  36.287  < 2e-16 ***
use          6.008e-05  1.653e-05   3.635 0.000299 ***
car          1.585e-02  2.187e-03   7.247 1.21e-12 ***
vel          1.054e+00  2.591e-01   4.068 5.31e-05 ***
l_temp      -5.104e-01  8.586e-02  -5.944 4.53e-09 ***
h_temp       1.661e-01  7.889e-02   2.106 0.035591 *  
h_hum        8.285e-02  1.811e-02   4.576 5.68e-06 ***
h_wind      -3.932e-01  1.532e-01  -2.567 0.010489 *  
mise         1.860e-01  1.696e-02  10.970  < 2e-16 ***
chomise      1.128e-01  2.893e-02   3.900 0.000106 ***
weekend      2.451e+00  6.603e-01   3.711 0.000224 ***
---
Signif. codes:  
0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 5.351 on 650 degrees of freedom
  (69 observations deleted due to missingness)
Multiple R-squared:  0.9073,	Adjusted R-squared:  0.9057 
F-statistic: 578.5 on 11 and 650 DF,  p-value: < 2.2e-16

※변수가 많아지면 R^2는 무조건 커진다. 따라서 변수도 잘 선택해야 함.
+Adjusted R-squared를 확인해야 함.

영가설 :   대립가설 : 
p-value에 따라 기각 -> 변수 유의
p-value에 따라 채택 -> 변수 유의 X

ⓕ <최소 제곱법>


to minimize 
-> 정규방정식 



-> 이렇게 해서 적합된  을 회귀 직선이라고 한다.

ⓖ 모델의 적합도 시험

(SST = SSR + SSE 증명)



ⓘ 계산 복잡도
모형이 데이터에 안 맞는 정도를 나타내는 함수를 손실 함수(loss function)이라고 부른다. 가장 널리 쓰이는 손실 함수는 평균 제곱 오차(mean squared error, 이하 MSE)이다. 예측과 실제의 차이를 오차라고 한다. 이 오차를 모두 제곱한 다음에 평균 낸 것이 MSE이다. MSE는 데이터가 예측으로부터 얼마나 퍼져있는지를 나타낸다고 할 수 있다.

물론 제곱 대신 절대값을 사용할 수도 있다. 절대값은 단순히 +, - 부호만 뗀 것이다. 이것을 평균 절대 오차(mean absolute error, 이하 MAE)라고 부른다.

MSE는 오차를 제곱하기 때문에 오차가 커지면 커질 수록 손실이 제곱으로 커진다. 따라서 100개의 예측 중에 99개가 잘 맞아도 하나에서 크게 틀리면 그 하나에 영향을 많이 받게 된다. 그런데 그 하나는 데이터가 잘못된 것일 수도 있고, 우연히 패턴에서 벗어난 것일 수도 있다. 이런 예외적인 데이터를 이상점(outlier)라고 한다. MAE는 MSE에 비해 이상점에 영향을 적게 받는다. 이렇게 이상점에 적게 영향 받는 것을 통계용어로는 강건(robust)하다고 한다. 즉, MAE는 강건한 특성이 있다.

-> 계산 복잡도란 변수가 너무 많고 훈련 셈플이 너무 많아 메모리에 모두 담을 수 없을 때 복잡하다고 한다.

-> 주된 표현법은 대문자 O 표현법을 따른다. 점근 표현법이라고도 부르는데 어떤 함수의 증가 양상을 다른 함수와 비교료 표현하는 방법이다. 복잡도를 단순화 할 때나 무한급수의 뒷부분을 간소화 할 때 쓰인다.


*경사를 이용한 알고리즘.
 데이터가 방대하여 컴퓨터의 메모리로 처리할 수 없을 때 사용
 요약 : 파라미터 벡터 theta와 비용함수의 그래프가 항상 볼록함수 이므로 최소가 되는 지점을 찾는다 
-> 변수 앞의 계수인 beta를 업데이트 하는 알고리즘을 이용하여 비용함수     의 전역 최솟값에 접근하게 하는 방법.
-> 업데이트 되는 알고리즘을 사용 즉 머신러닝의 영역
-> 최적의 알고리즘을 찾아야 함.



*경사 하강법
ⓐ비용 함수를 최소화하기 위해 반복해서 파라미터를 조정해가는 것.
그래디언트 : 비용함수의 미분 값.

ⓑwhy 미분값? -> 비용 함수(MSE)가 볼록 함수이기 때문에 미분=0이 되는 
                포인트가 최소값.

ⓒ학습률 하이퍼 파라미터와 시간과의 관계
학습률이 낮으면 시간이 오래걸리고
학습률이 높으면 알고리즘의 발산
-> 알고리즘의 발산 : 도함수가 1차이기 때문에 최소 포인트 좌우로 이동 반복
-> 임의 파라미터 벡터 포인트가 도함수에서 - + - + - + 값으로 발산

ⓓ두 가지 문제점 : 시작 포인트에 따라 전역 최솟값에 도달 X
                시간이 오래 걸림

ⓔ 표준화를 해야 하는 이유 : ‘평지’를 없애기 위해

*배치 경사 하강법
why 편도 함수?
-> theta_j를 고정했을 때의 MSE(비용함수)의 변화율

※알고리즘의 의미
theta에서 를 뺀다는 의미는 theta를 다음 step에서 더 작게 해준다는 의미. 알고리즘이다. 하여 얼마나 빼줄지에 대해 (다음 스텝에서 얼마나 내려가는 지에 대해) 정의해 주기 위해 벡터 eta를 곱해준다.

-단점 : 데이터가 커질 시에 속도가 매우 느려진다.

*확률적 경사 하강법
전체 데이터가 아닌 임의로 선택된 하나의 데이터 포인트에서 그래디언트를 계산 

-> 비용 함수가 불규칙할 경우 배치가 아닌 확률 경사 하강법 사용
-> 알고리즘이 지역 최솟값을 건너뛰게 도와주므로

장점 : 매우 큰 데이터에서 경사 하강법 사용 가능
     사용 메모리가 적음.
단점 : 배치 경사 하강법보다 불안정
       (좋은 파라미터이긴 하나 최적치가 아님)
해결 : 학습률의 조정
       큰 학습률에서 작은 학습률로 수정

*학습 곡선
X축은 훈련 수 Y축은 학습도 (error로 해도 같음)
주로 S자 모양이 일반적
모델을 선택하는데 기준이 됨
과소 적합인지 과대 적합인지 알 수 있다.
과소 적합인 경우에는 꽤 높은 error로 수렴하며 초기 검증 데이터에서 매우 높은 error을 보이다 감소
